{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-643ca19f8c23>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-643ca19f8c23>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    import tensor flow as tf\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#IMPORTING DEPENDENCIES\n",
    "#import input_data\n",
    "#min = input_data.read_data_sets(\"/tmp/data, one_hot=True\")\n",
    "import tensor flow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import numpy as np\n",
    "\n",
    "import keras as ks\n",
    "#from tensorflow import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-40.0 degrees celsius = -40.0 degrees fahrenhet\n",
      "-10.0 degrees celsius = 14.0 degrees fahrenhet\n",
      "0.0 degrees celsius = 32.0 degrees fahrenhet\n",
      "8.0 degrees celsius = 46.0 degrees fahrenhet\n",
      "15.0 degrees celsius = 59.0 degrees fahrenhet\n",
      "22.0 degrees celsius = 72.0 degrees fahrenhet\n",
      "38.0 degrees celsius = 100.0 degrees fahrenhet\n"
     ]
    }
   ],
   "source": [
    "#traninng\n",
    "x =np.array([-40, -10, 0, 8, 15, 22, 38], dtype=float)\n",
    "y =np.array([-40, 14, 32, 46, 59, 72, 100], dtype=float)\n",
    "\n",
    "for i,c in enumerate(x):\n",
    "    print(\"{} degrees celsius = {} degrees fahrenhet\".format(c, y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our layer\n",
    "#i.e specifying how many internal variable a layer will have\n",
    "\n",
    "#l0 = tf.ks.layers.Dense(units=1, input_shape=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9970c05b96ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "min= load_dataset('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l0 = tf.keras.layers.Dense(units=1, input_shape=[1]) \n",
    "model = tf.keras.Sequential([l0])\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))\n",
    "history = model.fit(celsius_q, fahrenheit_a, epochs=500, verbose=False)\n",
    "model.predict([100.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recommendation involves\n",
    "#collaborative system ie recommendation based on wat others has liked in the past\n",
    "#content based system ie based on wat u hv liked in the past\n",
    "# numpy and scipy will help with the maths\n",
    "# and lightfm(its a big library so only need libraries are imported) is  a model used for recommendation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lightfm import LightFM\n",
    "from lightfm.datasets import fetch_movielens\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_movielens(min_rating=4.0)\n",
    "# min rating is the minimum ratin we'll want to include in our data i.e we're coleting the movies for 4 or heigher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_feature_labels': array(['Toy Story (1995)', 'GoldenEye (1995)', 'Four Rooms (1995)', ...,\n",
       "        'Sliding Doors (1998)', 'You So Crazy (1994)',\n",
       "        'Scream of Stone (Schrei aus Stein) (1991)'], dtype=object),\n",
       " 'item_features': <1682x1682 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 1682 stored elements in Compressed Sparse Row format>,\n",
       " 'item_labels': array(['Toy Story (1995)', 'GoldenEye (1995)', 'Four Rooms (1995)', ...,\n",
       "        'Sliding Doors (1998)', 'You So Crazy (1994)',\n",
       "        'Scream of Stone (Schrei aus Stein) (1991)'], dtype=object),\n",
       " 'test': <943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
       " \twith 5469 stored elements in COOrdinate format>,\n",
       " 'train': <943x1682 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 49906 stored elements in COOrdinate format>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 49906 stored elements in COOrdinate format>\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 5469 stored elements in COOrdinate format>\n"
     ]
    }
   ],
   "source": [
    "#our fetch_movielens stores our data as string so lets print out the testing and training data\n",
    "print(repr(data['train']))\n",
    "print(repr(data['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the loss function it measures the differnce btw or model prediction and the desired output\n",
    "# we'll minimize it during training so our model gets more acurate over time\n",
    "# creating model\n",
    "model = LightFM(loss='warp') # a loss function warp = weighted approximate rank pairwise it looks at the rating of each useers \n",
    "# and predict ratings for each\n",
    "\n",
    "# content based + collaborative system = HYBRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x5021cf3d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model we'll use the fit method which takes 3 parameters the number of data , number of epoch and number of threads\n",
    "model.fit(data['train'], epochs=30, num_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 3\n",
      "   known positives:\n",
      "           Seven (Se7en) (1995)\n",
      "           Contact (1997)\n",
      "           Starship Troopers (1997)\n",
      "   Recommendation:\n",
      "         Air Force One (1997)\n",
      "         Contact (1997)\n",
      "         Conspiracy Theory (1997)\n",
      "User 25\n",
      "   known positives:\n",
      "           Dead Man Walking (1995)\n",
      "           Star Wars (1977)\n",
      "           Fargo (1996)\n",
      "   Recommendation:\n",
      "         English Patient, The (1996)\n",
      "         Fargo (1996)\n",
      "         Contact (1997)\n",
      "User 450\n",
      "   known positives:\n",
      "           Contact (1997)\n",
      "           George of the Jungle (1997)\n",
      "           Event Horizon (1997)\n",
      "   Recommendation:\n",
      "         I Know What You Did Last Summer (1997)\n",
      "         Scream (1996)\n",
      "         Scream 2 (1997)\n"
     ]
    }
   ],
   "source": [
    "# lest now generate a recommendation from our model by creating a dictionary that carries three parameters\n",
    "def samlpe_recommendation(model, data, user_ids):\n",
    "    \n",
    "    #number of users and items but movies in this case in training data \n",
    "    n_users, n_items = data['train'].shape\n",
    "    # creating a for loop to iterate through every user ids we import and we want report for each\n",
    "    for user_id in user_ids:\n",
    "        \n",
    "        # lightfm considers moves of 5 and above rating as positive to make the problem binary\n",
    "        # movies already liked\n",
    "        # tocsr()[user_id].indices is a sub array which we retrive using the indices attribute \n",
    "        known_positives = data['item_labels'][data['train'].tocsr()[user_id].indices]\n",
    "        \n",
    "        #we'll get the positives from the data in COMPPRESED SPARSE ROW FORMAT\n",
    "        \n",
    "        #to generate recommendation and store them in the score variable using the predictive method of our model\n",
    "        # movies our  model predicts they will like\n",
    "        \n",
    "        # user id as d 1st parameter then list of each movie the numpy gives us movies from zero to the number of items so we can predict the score of each item\n",
    "        # then store them in order of their score\n",
    "        scores = model.predict(user_id, np.arange(n_items))\n",
    "        \n",
    "        # rank them in order of most liked to least liked\n",
    "        top_items = data['item_labels'][np.argsort(-scores)]\n",
    "        # lets print the user id\n",
    "        print(\"User %s\" % user_id)\n",
    "        print(\"   known positives:\")\n",
    "        #top 3 known positive movies the user picked\n",
    "        for x in known_positives[:3]:\n",
    "            #creating a for loop ending in the third index\n",
    "            print(\"           %s\" % x)\n",
    "        \n",
    "        # creating our recommendation and top tree movies our model predicts\n",
    "        print(\"   Recommendation:\")\n",
    "        \n",
    "        for x in top_items[:3]:\n",
    "            print(\"         %s\" % x)\n",
    "            \n",
    "           # lets use a ransom number of uder ids \n",
    "samlpe_recommendation(model, data, [3, 25, 450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
